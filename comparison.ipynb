{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1 align = \"center\" >HRV analysis</h1>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdd4cbd53696501b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "The aim of this notebook is to perform the HRV analysis on the data from the BrainLab database. The data is stored as a CSV file. First the data will be loaded and then the HRV analysis will be performed. Data will be loaded using the `pandas` library. HRV analysis will be performed using the `pyhrv` and `neurokit2` libraries. `neurokit2` library is a Python library for biosignal processing. The data will be plotted using the `matplotlib` library."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "308fed2f97697ed1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing the libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aebc196bafe78b2d"
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [],
   "source": [
    "import neurokit2 as nk \n",
    "from pyhrv import frequency_domain as fd \n",
    "from pyhrv import time_domain as td\n",
    "import matplotlib.pyplot as plt \n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import venv\n",
    "from dotenv import load_dotenv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T01:18:11.864203900Z",
     "start_time": "2024-03-04T01:18:11.826258900Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are interested in the following HRV parameters:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f5a70cce86d93e1"
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [],
   "source": [
    "content = [\n",
    "           'HRV_SDNN',\n",
    "           'HRV_RMSSD',\n",
    "           'HRV_MeanNN',\n",
    "           'HRV_pNN50',\n",
    "           'HRV_pNN20',\n",
    "           'HRV_ApEn',\n",
    "           'HRV_SampEn',\n",
    "           'HRV_FuzzyEn',\n",
    "           'HRV_MSEn',\n",
    "           'HRV_ShanEn'\n",
    "           ]\n",
    "\n",
    "freq_content = [\n",
    "                'HRV_LF',\n",
    "                'HRV_HF',\n",
    "                'HRV_TP'\n",
    "                ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T01:18:11.878053600Z",
     "start_time": "2024-03-04T01:18:11.830757500Z"
    }
   },
   "id": "bb90076ee65a4ed1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c898b37e17f2c037"
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\damia\\OneDrive\\Pulpit\\Sonata\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "path = os.getenv('PATH_TO_FIlE')\n",
    "print(path)\n",
    "folder_path = Path(path) # path to the folder with the data\n",
    "csv_files = list(folder_path.glob(\"*.csv\")) # list of all the csv files in the folder\n",
    "print(csv_files) # printing the list of files"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T01:20:01.485639Z",
     "start_time": "2024-03-04T01:20:01.480798300Z"
    }
   },
   "id": "63d52aab8a391679"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HRV analysis using ABP signal"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "897eed73d2e42d76"
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [],
   "source": [
    "final_df_abp = pd.DataFrame()\n",
    "\n",
    "for name in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(name, sep=';')  # loading the data\n",
    "        df_abp = df['abp_cnap[mmHg]'].apply(lambda x: float(x.replace(',', '.')))  # replacing commas with dots\n",
    "        name_without_csv = os.path.splitext(os.path.basename(name))[0]  # getting the name of the file without the .csv extension\n",
    "\n",
    "        processed_signal = nk.ppg_process(df_abp, sampling_rate=200)  # processing the signal\n",
    "        hrv = nk.hrv(processed_signal[0], sampling_rate=200)  # performing the HRV analysis\n",
    "        interesting_content = hrv[content]  # selecting the interesting content\n",
    "\n",
    "        # freq content needs to be calculated separately because of the different sampling rate\n",
    "        hrv_freq = hrv[freq_content]  # selecting the interesting content\n",
    "\n",
    "        cleaned_signal = nk.ppg_clean(df_abp, sampling_rate=200)  # cleaning the signal\n",
    "        r_peaks = nk.ppg_findpeaks(cleaned_signal, sampling_rate=200)['PPG_Peaks']  # finding the R-peaks\n",
    "        pyhrv_freq = fd.welch_psd(rpeaks=r_peaks, show=False)['fft_abs']  # using pyhrv library to calculate the frequency domain parameters\n",
    "\n",
    "        merged_df = pd.concat([interesting_content, hrv_freq], axis=1)  # merging the dataframes\n",
    "        merged_df['file_name'] = name_without_csv  # adding file_name column\n",
    "        final_df_abp = pd.concat([final_df_abp, merged_df], ignore_index=True)  # concatenate final_df with merged_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in file {name} --> {str(e)}\")\n",
    "\n",
    "final_df_abp.to_csv('final_output.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T01:18:11.931853500Z",
     "start_time": "2024-03-04T01:18:11.855722600Z"
    }
   },
   "id": "9fb86212989eb5b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the data - ECG signal"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27db3566fbeb09d1"
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "print(final_df_abp.to_csv(index = False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T01:18:11.942824400Z",
     "start_time": "2024-03-04T01:18:11.863206800Z"
    }
   },
   "id": "e3c52c708c6d13e5"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "19fe0eb6f65fba7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HRV analysis using ECG signal"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50ef56102366f939"
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [],
   "source": [
    "final_df_ecg = pd.DataFrame()\n",
    "for name in csv_files:\n",
    "    # here i need to adjust the code to search for the ECG signal in the csv file \n",
    "    try:\n",
    "        df = pd.read_csv(name, sep=';')\n",
    "        if 'ecg' in df.columns:\n",
    "            df_ecg = df['ecg']\n",
    "        elif 'ecg[mV]' in df.columns:\n",
    "            df_ecg = df['ecg[mV]']\n",
    "        else:\n",
    "            print(f\"No 'ecg' or 'ecg[mV]' column in file {name}\")\n",
    "            continue\n",
    "            \n",
    "        df_ecg = [float(i.replace(',','.')) for i in df_ecg]\n",
    "        processed_signal = nk.ecg_process(df_ecg, sampling_rate=200)\n",
    "        hrv = nk.hrv(processed_signal[0], sampling_rate=200)\n",
    "        name_without_csv = os.path.splitext(name.name)[0]\n",
    "        interesting_content = hrv[content]\n",
    "        # print(f\"------------{name_without_csv}------------\")\n",
    "        # print(interesting_content)\n",
    "        \n",
    "        hrv_freq = hrv[freq_content]\n",
    "        # print(hrv_freq)\n",
    "        \n",
    "        cleaned_signal = nk.ecg_clean(df_ecg, sampling_rate=200)\n",
    "        r_peaks = nk.ecg_findpeaks(cleaned_signal, sampling_rate=200)['ECG_R_Peaks']\n",
    "        pyhrv_freq = fd.welch_psd(rpeaks=r_peaks, show=False)['fft_abs']\n",
    "        # print(pyhrv_freq)\n",
    "        merged_df = pd.concat([interesting_content, hrv_freq], axis=1)\n",
    "        merged_df['file_name'] = name_without_csv\n",
    "        # print(merged_df)\n",
    "        all_analyse = merged_df.to_csv(index = False)\n",
    "        final_df_ecg = pd.concat([final_df_ecg, merged_df], ignore_index=True)  # concatenate final_df with merged_df\n",
    "        \n",
    "        \n",
    "    except Exception as e :\n",
    "        print(f\"Error in file {name} --> {str(e)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T01:18:11.943821800Z",
     "start_time": "2024-03-04T01:18:11.872006400Z"
    }
   },
   "id": "fcd3baad2d536d10"
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "print(final_df_ecg.to_csv(index = False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T01:18:11.943821800Z",
     "start_time": "2024-03-04T01:18:11.877056500Z"
    }
   },
   "id": "c005de0dd3d7e5e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Plotting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "529846450d8339dd"
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'file_name'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_17272\\1090678030.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mfinal_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmerge\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfinal_df_abp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfinal_df_ecg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mon\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'file_name'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msuffixes\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'_abp'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'_ecg'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001B[0m\n\u001B[0;32m    166\u001B[0m             \u001B[0mvalidate\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvalidate\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    167\u001B[0m             \u001B[0mcopy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    168\u001B[0m         )\n\u001B[0;32m    169\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 170\u001B[1;33m         op = _MergeOperation(\n\u001B[0m\u001B[0;32m    171\u001B[0m             \u001B[0mleft_df\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    172\u001B[0m             \u001B[0mright_df\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    173\u001B[0m             \u001B[0mhow\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mhow\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001B[0m\n\u001B[0;32m    790\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mright_join_keys\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    791\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin_names\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    792\u001B[0m             \u001B[0mleft_drop\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    793\u001B[0m             \u001B[0mright_drop\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 794\u001B[1;33m         ) = self._get_merge_keys()\n\u001B[0m\u001B[0;32m    795\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    796\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mleft_drop\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    797\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mleft\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mleft\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_drop_labels_or_levels\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mleft_drop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1293\u001B[0m                         \u001B[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1294\u001B[0m                         \u001B[1;31m#  the latter of which will raise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1295\u001B[0m                         \u001B[0mrk\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mHashable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrk\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1296\u001B[0m                         \u001B[1;32mif\u001B[0m \u001B[0mrk\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1297\u001B[1;33m                             \u001B[0mright_keys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mright\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_label_or_level_values\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrk\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1298\u001B[0m                         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1299\u001B[0m                             \u001B[1;31m# work-around for merge_asof(right_index=True)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1300\u001B[0m                             \u001B[0mright_keys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mright\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1906\u001B[0m             \u001B[0mvalues\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mxs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mother_axes\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1907\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_is_level_reference\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1908\u001B[0m             \u001B[0mvalues\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maxes\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_level_values\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1909\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1910\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1911\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1912\u001B[0m         \u001B[1;31m# Check for duplicates\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1913\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mvalues\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'file_name'"
     ]
    }
   ],
   "source": [
    "final_df = pd.merge(final_df_abp, final_df_ecg, on='file_name', suffixes=('_abp', '_ecg'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T01:18:11.944819100Z",
     "start_time": "2024-03-04T01:18:11.887344500Z"
    }
   },
   "id": "f723e3713931b24b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(final_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-04T01:18:11.920883400Z"
    }
   },
   "id": "40447f78c67a57fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To present plot of data we will use the bland altman plot. The Bland-Altman plot is a graphical method to analyze the differences between two methods of measurement. The plot shows the difference between the two measurements on the y-axis and the average of the two measurements on the x-axis. The plot is used to determine if the two methods are in agreement, or if they disagree in a consistent way."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a75e3226640bab54"
  },
  {
   "cell_type": "markdown",
   "source": [
    "find matching files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2432389f37e9e01f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
    "    data1 = np.asarray(data1)\n",
    "    data2 = np.asarray(data2)\n",
    "    mean = np.mean([data1, data2], axis=0)  # Mean of the data\n",
    "    diff = data1 - data2  # Difference between data1 and data2\n",
    "    md = np.mean(diff)  # Mean of the difference\n",
    "    sd = np.std(diff, axis=0)  # Standard deviation of the difference\n",
    "\n",
    "    plt.scatter(mean, diff, *args, **kwargs)  # Use mean1 directly as x-values\n",
    "    plt.axhline(md, color='gray', linestyle='--', label='Mean Difference')  # Mean Difference line\n",
    "    plt.axhline(md + 1.96 * sd, color='red', linestyle='--', label='+ 1.96 SD')\n",
    "    plt.axhline(md - 1.96 * sd, color='red', linestyle='--', label='- 1.96 SD')\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "    plt.xlabel('Mean of the two measurements')\n",
    "    plt.ylabel('Difference between the two measurements')\n",
    "    \n",
    "    plt.text(1.15, 0.5, f'Mean Difference: {md:.2f}\\nSD of Difference: {sd:.2f}', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "#   code from https://stackoverflo.com/questions/16399279/bland-altman-plot-in-python - edited"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-04T01:18:11.922878600Z"
    }
   },
   "id": "23566d70663865b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in content:\n",
    "    plt.figure()\n",
    "    bland_altman_plot(final_df[i + '_abp'], final_df[i + '_ecg'])\n",
    "    plt.title(f'Bland-Altman Plot for {i}')\n",
    "    plt.savefig(f'Bland_Altman_Plot_{i}.png')\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-04T01:18:11.925870200Z"
    }
   },
   "id": "5904f6474937feb6"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d37680bc452edac7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
